{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats // 2 + 1  # todo even vrsos odd\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim * 2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train and test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from jiwer import wer\n",
    "import Model\n",
    "import Utils\n",
    "from ASR import WB\n",
    "\n",
    "\n",
    "def train(model, device, batch_iterator, criterion, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    data_len = len(batch_iterator)\n",
    "    for batch_idx, _data in enumerate(batch_iterator):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)  # using log_softmax instead of softmax for numerical stability\n",
    "        output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, torch.from_numpy(input_lengths), torch.from_numpy(label_lengths))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if batch_idx % 50 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                       100. * batch_idx / data_len, loss.item()))\n",
    "            if WB:\n",
    "                wandb.log({\"train_loss\": loss.item()})\n",
    "                decoded_preds, decoded_targets = Utils.greedy_decoder(output.transpose(0, 1), labels,\n",
    "                                                                      label_lengths)\n",
    "                wer_sum = 0\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    wer_sum += wer(decoded_targets[j], decoded_preds[j])\n",
    "\n",
    "                wandb.log({\"train_wer\": wer_sum / len(decoded_preds)})\n",
    "\n",
    "\n",
    "def validation(model, device, val_loader, criterion, epoch):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_wer = []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(val_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, torch.from_numpy(input_lengths), torch.from_numpy(label_lengths))\n",
    "            val_loss += loss.item() / len(val_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = Utils.greedy_decoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                val_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "    avg_wer = sum(val_wer) / len(val_wer)\n",
    "    # experiment.log_metric('val_loss', val_loss, step=iter_meter.get())\n",
    "    # experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
    "\n",
    "    print(\n",
    "        'val set: Average loss: {:.4f}, Average WER: {:.4f}\\n'.format(val_loss, avg_wer))\n",
    "\n",
    "    # print a sample of the val data and decoded predictions against the true labels\n",
    "    if epoch % 10 == 0:\n",
    "        print('Ground Truth -> Decoded Prediction')\n",
    "        for i in range(10):\n",
    "            print('{} -> {}'.format(decoded_targets[i], decoded_preds[i]))\n",
    "\n",
    "    if WB:\n",
    "        wandb.log({\"val_loss\": val_loss})\n",
    "        wandb.log({\"val_wer\": avg_wer})\n",
    "\n",
    "\n",
    "def train_and_validation(hparams, batch_iterators):\n",
    "    train_loader = batch_iterators[0]\n",
    "    val_loader = batch_iterators[1]\n",
    "    epochs = hparams['epochs']\n",
    "\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = Model.SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
    "                                              steps_per_epoch=len(train_loader),  # todo: check if this is correct\n",
    "                                              epochs=hparams['epochs'],\n",
    "                                              anneal_strategy='linear')\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "        validation(model, device, val_loader, criterion, epoch)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')\n",
    "\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "\n",
    "def greedy_decoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    \"\"\"\n",
    "    Greedy Decoder. Decodes the output of the network by picking the label with the highest probability at each time step.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j - 1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from Utils import TextTransform\n",
    "from HyperParameters import hparams\n",
    "\n",
    "\n",
    "class BatchIterator:\n",
    "    def __init__(self, x, y, input_lengths, label_lengths, batch_size, shuffle=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.input_lengths = input_lengths\n",
    "        self.label_lengths = label_lengths\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = len(x)\n",
    "        self.num_batches = (self.num_samples + batch_size - 1) // batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            indices = np.random.permutation(self.num_samples)\n",
    "            self.x = self.x[indices]\n",
    "            self.y = self.y[indices]\n",
    "            self.input_lengths = np.asarray(self.input_lengths)[indices]\n",
    "            self.label_lengths = np.asarray(self.label_lengths)[indices]\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch < self.num_batches:\n",
    "            start_idx = self.current_batch * self.batch_size\n",
    "            end_idx = min((self.current_batch + 1) * self.batch_size, self.num_samples)\n",
    "\n",
    "            batch_x = self.x[start_idx:end_idx]\n",
    "            batch_y = self.y[start_idx:end_idx]\n",
    "            batch_input_lengths = self.input_lengths[start_idx:end_idx]\n",
    "            batch_label_lengths = self.label_lengths[start_idx:end_idx]\n",
    "\n",
    "            self.current_batch += 1\n",
    "\n",
    "            return batch_x, batch_y, batch_input_lengths, batch_label_lengths\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "\n",
    "def load_wavs_data(load_again=False, save=False,\n",
    "                   path=r\".\\an4\"):\n",
    "    text_transform = TextTransform()\n",
    "\n",
    "    if load_again:\n",
    "        all_spectrogram = torch.load(\"data/all_spectrogram.pt\")\n",
    "        all_labels = torch.load(\"data/all_labels.pt\")\n",
    "        all_input_lengths = torch.load(\"data/all_input_lengths.pt\")\n",
    "        all_label_lengths = torch.load(\"data/all_label_lengths.pt\")\n",
    "        return all_spectrogram, all_labels, all_input_lengths, all_label_lengths\n",
    "\n",
    "    valid_file = [\"test\", \"train\", \"val\"]\n",
    "    all_spectrogram = {\"test\": [], \"train\": [], \"val\": []}\n",
    "    all_labels = {\"test\": [], \"train\": [], \"val\": []}\n",
    "    all_input_lengths = {\"test\": [], \"train\": [], \"val\": []}\n",
    "    all_label_lengths = {\"test\": [], \"train\": [], \"val\": []}\n",
    "    for dir in os.listdir(path):\n",
    "        if dir in valid_file:\n",
    "            spectrogram = []\n",
    "            labels = []\n",
    "            input_lengths = []\n",
    "            label_lengths = []\n",
    "            for root2, dirs2, files2 in os.walk(os.path.join(path, dir)):\n",
    "                for file in files2:\n",
    "                    if file.endswith(\".txt\"):\n",
    "                        # change suffix to wav\n",
    "                        wav = file.replace(\".txt\", \".wav\")\n",
    "                        # print(os.path.join(root2, file))\n",
    "                        # change last dir to wav instead of txt\n",
    "                        root2_wav = root2.replace(\"txt\", \"wav\")\n",
    "\n",
    "                        # load wav:\n",
    "                        waveform, sample_rate = torchaudio.load(os.path.join(root2_wav, wav))\n",
    "                        # mfcc:\n",
    "                        mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate, n_mfcc=13)(waveform)\n",
    "                        mfcc = mfcc.squeeze(0)\n",
    "                        mfcc = mfcc.transpose(0, 1)\n",
    "                        # add mfcc to y:\n",
    "                        spectrogram.append(mfcc)\n",
    "\n",
    "                        # load txt:\n",
    "                        with open(os.path.join(root2, file), 'r') as f:\n",
    "                            text = f.read()\n",
    "                            int_text = text_transform.text_to_int(text.lower())\n",
    "                        # add text to labels:\n",
    "                        label = torch.Tensor(int_text)\n",
    "                        labels.append(label)\n",
    "                        input_lengths.append(mfcc.shape[0] // 2)  # todo why divide by 2?\n",
    "                        label_lengths.append(len(label))\n",
    "                    # print(file)\n",
    "\n",
    "            # todo maybe the padding should be done for all the data together (train test and val)\n",
    "            spectrogram = nn.utils.rnn.pad_sequence(spectrogram, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "            labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "            all_spectrogram[dir] = spectrogram\n",
    "            all_labels[dir] = labels\n",
    "            all_input_lengths[dir] = input_lengths\n",
    "            all_label_lengths[dir] = label_lengths\n",
    "\n",
    "    if save:\n",
    "        torch.save(all_spectrogram, \"data/all_spectrogram.pt\")\n",
    "        torch.save(all_labels, \"data/all_labels.pt\")\n",
    "        torch.save(all_input_lengths, \"data/all_input_lengths.pt\")\n",
    "        torch.save(all_label_lengths, \"data/all_label_lengths.pt\")\n",
    "    return all_spectrogram, all_labels, all_input_lengths, all_label_lengths\n",
    "\n",
    "\n",
    "def get_batch_iterator(data_type, batch_size=hparams[\"batch_size\"]):\n",
    "    if data_type not in [\"test\", \"train\", \"val\"]:\n",
    "        raise ValueError(\"data_type must be one of [test, train, val]\")\n",
    "    all_spectrogram, all_labels, all_input_lengths, all_label_lengths = load_wavs_data(load_again=True, save=True)\n",
    "    return BatchIterator(all_spectrogram[data_type], all_labels[data_type],\n",
    "                         all_input_lengths[data_type], all_label_lengths[data_type], batch_size)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Main"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/86 (0%)]\tLoss: 21.304291\n",
      "Train Epoch: 1 [500/86 (58%)]\tLoss: 24.125551\n",
      "\n",
      "evaluating...\n",
      "val set: Average loss: 12.1804, Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 2 [0/86 (0%)]\tLoss: 10.630229\n",
      "Train Epoch: 2 [500/86 (58%)]\tLoss: 4.593483\n",
      "\n",
      "evaluating...\n",
      "val set: Average loss: 3.5923, Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 3 [0/86 (0%)]\tLoss: 3.419579\n",
      "Train Epoch: 3 [500/86 (58%)]\tLoss: 3.252816\n",
      "\n",
      "evaluating...\n",
      "val set: Average loss: 3.3025, Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 4 [0/86 (0%)]\tLoss: 3.477070\n",
      "Train Epoch: 4 [500/86 (58%)]\tLoss: 3.304673\n",
      "\n",
      "evaluating...\n",
      "val set: Average loss: 3.1093, Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 5 [0/86 (0%)]\tLoss: 3.081306\n",
      "Train Epoch: 5 [500/86 (58%)]\tLoss: 3.087465\n",
      "\n",
      "evaluating...\n",
      "val set: Average loss: 3.0277, Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 6 [0/86 (0%)]\tLoss: 3.054180\n",
      "Train Epoch: 6 [500/86 (58%)]\tLoss: 3.232636\n",
      "\n",
      "evaluating...\n",
      "val set: Average loss: 2.9881, Average WER: 1.0000\n",
      "\n",
      "Train Epoch: 7 [0/86 (0%)]\tLoss: 3.249535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 49\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m WB:\n\u001B[0;32m     46\u001B[0m         wandb\u001B[38;5;241m.\u001B[39mfinish()\n\u001B[1;32m---> 49\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 43\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     40\u001B[0m val_batch_iterator \u001B[38;5;241m=\u001B[39m DataLoader\u001B[38;5;241m.\u001B[39mget_batch_iterator(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m, hparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     41\u001B[0m all_iterators \u001B[38;5;241m=\u001B[39m [train_batch_iterator, test_batch_iterator, val_batch_iterator]\n\u001B[1;32m---> 43\u001B[0m \u001B[43mTrainAndEvaluation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_and_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_iterators\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m WB:\n\u001B[0;32m     46\u001B[0m     wandb\u001B[38;5;241m.\u001B[39mfinish()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Audio-And-Speech-Processing\\final project\\TrainAndEvaluation.py:106\u001B[0m, in \u001B[0;36mtrain_and_validation\u001B[1;34m(hparams, batch_iterators)\u001B[0m\n\u001B[0;32m    100\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mOneCycleLR(optimizer, max_lr\u001B[38;5;241m=\u001B[39mhparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    101\u001B[0m                                           steps_per_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_loader),  \u001B[38;5;66;03m# todo: check if this is correct\u001B[39;00m\n\u001B[0;32m    102\u001B[0m                                           epochs\u001B[38;5;241m=\u001B[39mhparams[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    103\u001B[0m                                           anneal_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m--> 106\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     validation(model, device, val_loader, criterion, epoch)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Audio-And-Speech-Processing\\final project\\TrainAndEvaluation.py:26\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, device, batch_iterator, criterion, optimizer, scheduler, epoch)\u001B[0m\n\u001B[0;32m     23\u001B[0m output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (time, batch, n_class)\u001B[39;00m\n\u001B[0;32m     25\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, labels, torch\u001B[38;5;241m.\u001B[39mfrom_numpy(input_lengths), torch\u001B[38;5;241m.\u001B[39mfrom_numpy(label_lengths))\n\u001B[1;32m---> 26\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     29\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Audio-And-Speech-Processing\\venv\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Audio-And-Speech-Processing\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import TrainAndEvaluation\n",
    "import DataLoader\n",
    "import wandb\n",
    "from HyperParameters import hparams\n",
    "from HyperParameters import WB\n",
    "\n",
    "DESCRIPTION = 'initial work'\n",
    "RUN = 'Complex Model'\n",
    "\n",
    "\n",
    "def init_w_and_b():\n",
    "    epochs = hparams['epochs']\n",
    "    learning_rate = hparams['learning_rate']\n",
    "\n",
    "    if WB:\n",
    "        wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            group=\"Complex Model initial work\",\n",
    "            project=\"ASR\",\n",
    "            # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "            name=f\"{DESCRIPTION}{RUN}_{hparams}_epochs\",\n",
    "            notes='checking if log is work properly',\n",
    "            # Track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"architecture\": \"assembly\",\n",
    "                \"dataset\": \"AN4\",\n",
    "                \"epochs\": epochs,\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "def main():\n",
    "    if WB:\n",
    "        wandb.login()\n",
    "        init_w_and_b()\n",
    "\n",
    "    train_batch_iterator = DataLoader.get_batch_iterator(\"train\", hparams[\"batch_size\"])\n",
    "    test_batch_iterator = DataLoader.get_batch_iterator(\"test\", hparams[\"batch_size\"])\n",
    "    val_batch_iterator = DataLoader.get_batch_iterator(\"val\", hparams[\"batch_size\"])\n",
    "    all_iterators = [train_batch_iterator, test_batch_iterator, val_batch_iterator]\n",
    "\n",
    "    TrainAndEvaluation.train_and_validation(hparams, all_iterators)\n",
    "\n",
    "    if WB:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "main()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
